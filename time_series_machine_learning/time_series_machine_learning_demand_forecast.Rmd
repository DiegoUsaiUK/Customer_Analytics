---
title: "Time Series Machine Learning Analysis and Demand Forecasting with H2O & TSstudio"
subtitle: "How I used machine learning to implement a time series forecast of weekly revenue"
author: "Diego Usai"
date: "11 December 2019"
output:
  html_document:
    theme: spacelab
    # df_print: paged
    highlight: pygments
    number_sections: false
    toc: true
    toc_float: true
    toc_depth : 4
    font-family: Roboto
    code_folding: none
    keep_md: false
    dpi: 300
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  eval       = TRUE,   # TRUE to evaluate every single chunck
  warning    = FALSE,  # FALSE to suppress warnings from being shown
  message    = FALSE,  # FALSE to avoid package loading messages
  cache      = TRUE,   # TRUE to save every single chunck to a folder
  echo       = TRUE,   # TRUE to display code in output document
  out.width  = "90%",
  out.height = "70%",
  fig.align  = "center"
)
```


```{r switch off locale, include=FALSE}
# turn off locale-specific sorting for messages in English
Sys.setlocale("LC_TIME", "C")
```


## Introduction

Traditional approaches to time series analysis and forecasting, like [__Linear Regression__](https://en.wikipedia.org/wiki/Linear_regression), [__Holt-Winters Exponential Smoothing__](https://en.wikipedia.org/wiki/Exponential_smoothing), [__ARMA/ARIMA/SARIMA__](https://en.wikipedia.org/wiki/Autoregressive%E2%80%93moving-average_model) and [__ARCH/GARCH__](https://en.wikipedia.org/wiki/Autoregressive_conditional_heteroskedasticity), have been well-established for decades and find applications in fields as varied as __business and finance__ (e.g. predict stock prices and analyse trends in financial markets), the __energy sector__ (e.g. forecast electricity consumption) and __academia__ (e.g. measure socio-political phenomena).

In more recent times, the popularisation and wider availability of open source frameworks like [__Keras__](https://keras.io/), [__TensorFlow__](https://www.tensorflow.org/) and [__scikit-learn__](https://scikit-learn.org/) helped machine learning approaches like [__Random Forest__](https://en.wikipedia.org/wiki/Random_forest), [__Extreme Gradient Boosting__](https://en.wikipedia.org/wiki/XGBoost), [__Time Delay Neural Network__](https://en.wikipedia.org/wiki/Time_delay_neural_network) and [__Recurrent Neural Network__](https://en.wikipedia.org/wiki/Recurrent_neural_network) to gain momentum in time series applications. These techniques allow for historical information to be introduced as input to the model through a set of time delays. 

The advantage of using machine learning models over more traditional methods is that they can have `higher predictive power`, especially when predictors have a clear causal link to the response. Moreover, they can handle complex calculations over larger numbers of inputs much `faster`.

However, they tend to have a `wider array of tuning parameters`, are generally `more complex` than "classic" models, and can be `expensive` to fit, both in terms of computing power and time. To top it off, their `black box` nature makes their output harder to interpret and has given birth to the ever growing field of [__Machine Learning interpretability__](https://www.h2o.ai/wp-content/uploads/2019/08/An-Introduction-to-Machine-Learning-Interpretability-Second-Edition.pdf) (I am not going to touch on this as it's outside the scope of the project)

## Project structure

In this project I am going to explain in detail the various steps needed to model time series data with machine learning models.

These include:

- __exploratory time series analysis__

- __feature engineering__

- __models training and validation__

- __comparison of models performance and forecasting__

In particular, I use `TSstudio` to carry out a "traditional" time series __exploratory analysis__ to describe the time series and its components and show how to use the insight I gather to __create features__ for a machine learning pipeline to ultimately generate a weekly revenue forecast. 

For __modelling and forecasting__ I've chosen the high performance, open source machine learning library `H2O`. I am fitting an assorted selection of machine learning models such as __Generalised Linear Model__, __Gradient Boosting Machine__ and __Random Forest__ and also using __AutoML__ for _automatic machine learning_, one of the most exciting features of the `H2O` library. 

## The data

```{r Load libraries, message = FALSE}
library(tidyverse)
library(lubridate)
library(readr)
library(TSstudio)
library(scales)
library(plotly)
library(h2o)
library(vip)
library(gridExtra)
library(knitr)
```

The dataset I'm using here accompanies a [Redbooks publication](https://www.redbooks.ibm.com/abstracts/sg248133.html?Open) and is available as a free download in the [Additional Material](ftp://www.redbooks.ibm.com/redbooks/SG248133) section. The data covers __3 & 1/2 years__ worth of sales `orders` for the __Sample Outdoors Company__, a fictitious B2B outdoor equipment retailer enterprise and comes with details about the `products` they sell as well as their customers (which in their case are `retailers`). Due to its __artificial nature__, the series presents a few oddities and quirks, which I am going to point out throughout this project.

Here I'm simply loading up the compiled dataset but if you want to follow along I've also written a post called [Loading, Merging and Joining Datasets](https://diegousai.io/2019/09/loading-merging-and-joining-datasets/) where I show how I've assembled the various data feeds and sorted out the likes of variable naming, new features creation and some general housekeeping tasks.


```{r Importing Preclensed Data File, eval=FALSE, include=TRUE}
# Import orders
orders_tmp <- 
   read_rds("../00_data/orders_tbl.rds")
```


```{r Importing Data FULL PATH, eval=TRUE, include=FALSE}
# Import orders
orders_tmp <- 
   read_rds("C:/Users/LENOVO/Desktop/PROJECTS/Customer_Analytics/06_Demand_Forecast/00_data/orders_tbl.rds")
```

You can find the full code on [my Github repository](https://github.com/DiegoUsaiUK/Loading_Merging_and_Joining_Datasets).


### Initial exploration

Time series data have a set of unique features, like _timestamp_, _frequency_ and _cycle/period_, that have applications for both descriptive and predictive analysis. R provides several classes to represent time series objects (`xts` and `zoo` to name but the main ones), but to cover the __descriptive analysis__ for this project I've chosen to use the `ts` class, which is supported by the `TSstudio` library.  

`TSstudio` comes with some very useful functions for interactive visualization of time series objects. And I really like the fact that this library uses _plotly_ as its visualisation engine! 

First of all, I select the data I need for my analysis (`order_date` and `revenue` in this case) and aggregate it to a __weekly frequency__. I mark this dataset with a `_tmp` suffix to denote that is a _temporary_ version and a few steps are still needed before it can be used.

```{r}
revenue_tmp <- 
  orders_tmp %>% 
  # filter out final month of the series, which is incomplete
  filter(order_date <= "2007-06-25") %>% 
  select(order_date, revenue) %>%
  mutate(order_date = 
           floor_date(order_date, 
                     unit = 'week', 
                     # setting up week commencing Monday
                     week_start = getOption("lubridate.week.start", 1))) %>%
  group_by(order_date) %>%
  summarise(revenue   = sum(revenue)) %>%
  ungroup()
```

```{r,collapse=TRUE}
revenue_tmp %>% str()
```

The series spans across __3 & 1/2 years__ worth of sales `orders` so I should expect to have __at least 182 data point__ but there are __only 89 observations__!

Let's take a closer look to see what's happening:

```{r, collapse=TRUE}
revenue_tmp %>% head(10)
```


__A couple of things to notice here:__ this series presents an unusual weekly pattern, with sales logged twice a month on average. Also, the first week of 2004 has no sales logged against it. 

To carry out time series analysis with `ts` objects I need to make sure that I have a full 52 weeks in each year, which should also include weeks with no sales. 

So before converting my data to a `ts` object I need to:

- __Add 1 observation at the beginning of the series__ to insure that the first year includes 52 weekly observations

- __All is arranged in chronological order.__ This is especially important because the output may not be correctly mapped to the actual index of the series, leading to inaccurate results.

- __Fill the gaps in incomplete datetime variables.__ The `pad` function from the `padr` library inserts a record for each of the missing time points (the default fill value is NA)

- __Replace missing values with a zero__ as there are no sales recorded against the empty weeks. The `fill_by_value` function from the `padr` library helps with that.


```{r}
revenue_tbl <- 
  revenue_tmp %>% 
    rbind(list('2004-01-05', NA, NA)) %>%
    arrange(order_date) %>% 
    padr::pad() %>% 
    padr::fill_by_value(value = 0) 
```

```{r, collapse=TRUE}
revenue_tbl %>% summary()
```

Now I can take a look at __weekly revenue__, my response variable
```{r}
(revenue_tbl %>% 
  ggplot(aes(x = order_date, y = revenue)) + 
  geom_line(colour = 'darkblue') +
  theme_light() +
  scale_y_continuous(labels = scales::dollar_format(scale = 1e-6, 
                                                    suffix = "m")) +
  labs(title = 'Weekly Revenue - 2004 to June 2007',
           x = "",
           y = 'Revenue ($m)')
) %>% ggplotly()
```

As already mentioned, the series is artificially generated and does not necessarily reflect what would happen in a real-life situation. If this was an actual analytics consulting project, I would most definitely question the odd weekly sales frequency with my client. 

But assuming that this is the real deal, __the challenge__ here is to construct a thought-through selection of __meaningful features__ and test them on __a few machine learning models__ to find one that can to __produce a good forecast__.

__CHALLENGE EXCEPTED!__

## Exploratory analysis

In this segment I am exploring the time series, examining its components and seasonality structure, and carry out a correlation analysis to identify the main characteristics of the series.

Before I can change my series into a `ts` object, I need to define the `start` (or `end`) argument of the series. I want the count of weeks to start with the first week of the year, which ensures that all aligns with the `ts` framework. 

```{r, collapse=TRUE}
start_point_wk <-  c(1,1)

start_point_wk
```

I create the `ts` object by selecting the response variable (`revenue`) as the data argument and specifying a frequency of 52 weeks. 

```{r}
ts_weekly <- 
  revenue_tbl %>% 
  # filter(order_date <= "2006-12-31") %>% 
  select(revenue) %>%
  ts(start = start_point_wk,
     frequency = 52)
```

```{r, collapse=TRUE}
ts_info(ts_weekly)
```

Checking the series attributes with the `ts_info()` function shows that the series is a weekly `ts` object with 1 variable and 181 observations.

### Time series components

Let's now plot our time series with the help of `TSstudio`'s graphic functions.

`ts_decompose` breaks down the series into its elements: __Trend__, __Seasonality__, and __Random__ components. 

```{r}
ts_decompose(ts_weekly, type = 'additive')
```

- __Trend__: The series does not appear to have a _cyclical component_ but shows a distinct _upward trend_. The trend is potentially _not linear_, which I will try to capture by including a squared trend element in the features. 

- __Seasonal__: the plot shows a distinct _seasonal pattern_, which I will explore next.

- __Random__: The _random component_ appears to be randomly distributed.

### Seasonal component

Let's now zoom in on the __seasonal component__ of the series

```{r}
ts_seasonal(ts_weekly, type = 'normal')
```
Although there are 12 distinct "spikes" (one for each month of the year), the plot does not suggest the presence of a __canonical seasonality__. However, with very few exceptions, sales are logged on the same weeks each year and I will try to capture that regularity with a feature variable.

### Correlation analysis

The __autocorrelation function (ACF)__ describes the level of correlation between the series and its lags. 

Due to the odd nature of the series at hand, the AC plot is not very straightforward to read and interpret: it shows that there is a lag structure but due to the noise in the series it's difficult to pick up a meaningful pattern.

```{r}
ts_acf(ts_weekly, lag.max = 52)
```

However, I can make use of __Lag Visualizations__ and play with the lag number to identify potential correlation between the series and it lags.

In this case, aligning the number of weeks to a __quarterly frequency__ shows a distinct __linear relationship__ with quarterly lags. For simplicity, I will only include __lag 13__ in the models to control for the effect of level of sales during the last quarter.
```{r}
ts_lags(ts_weekly, lags = c(13, 26, 39, 52))
```
Using the same trick reveals a __strong linear relationship__ with the __first yearly lag__. Again, I will include only a __lag 52__ in the models.
```{r}
ts_lags(ts_weekly, lags = c(52, 104, 156))
```

### Summary of exploratory analysis

- The series has a 2-week-on, 2-week-off purchase frequency with __no canonical seasonality__. However, sales are logged roughly on the same weeks each year, with very few exceptions.

- The series does not appear to have a __cyclical component__ but shows a clear __upward trend__ as well as potentially a __not linear trend__.

- ACF was difficult to interpret due to the noisy data but the lag plots hint at a __yearly__ and __quarterly lag structure__.


## Modelling 

1 - The __modelling and forecasting__ strategy is to:

- train and cross-validate all models up to and including __Q1 2007__ and compare their in-sample __predictive performance__ using __Q1 2007__. 

- pretend I do not have data for __Q2 2007__, generate a forecast for that period using all fitted models, and compare their __forecasts performance__ against __Q2 2007 actuals__

Below you find a visual representation of the strategy. I find from experience that supporting explanations with a good visualisation is a great way to bring your points to life, especially when working with time series analysis.

2 - All __models accuracy__ will be compared with __performance metrics__ and __actual vs predicted__ plots.

The __performance metrics__ I am going to be using are:

- __R^2__ is a _goodness-of-fit metric_ that explains in percentage terms the amount of variation in the response variable that is due to variation in the feature variables.

- __RMSE__ (or _Root Mean Square Error_) is the standard deviation of the residuals and measures the average magnitude of the prediction error. Basically, it tells you how spread out residuals are.

```{r}
revenue_tbl %>%
  filter(order_date >= "2005-01-03") %>% 
  ggplot(aes(order_date, revenue)) +
  geom_line(colour = 'black', size = 0.7) +
  geom_point(colour = 'black', size = 0.7) +
  geom_smooth(se = FALSE, colour = 'red', size = 1, linetype = "dashed") +
  theme_light() +
  scale_y_continuous(limits = c(0, 11.5e7),
                     labels = scales::dollar_format(scale = 1e-6, suffix = "m")) +
  labs(title    = 'Weekly Revenue - 2005 to June 2007',
       subtitle = 'Train, Test and Forecast Data Portions',
       x = "",
       y = 'Revenue ($m)') +
  
  # Train Portion
  annotate(x = ymd('2005-12-01'), y = (10.5e7), fill = 'black',
           'text',  label = 'Train\nPortion', size = 2.8) +
  
  # Test Portion
  annotate(x = ymd('2007-02-05'), y = (10.5e7),
           'text',  label = 'Test\nPortion', size = 2.8) +
  geom_rect(xmin = as.numeric(ymd('2006-12-18')),
            xmax = as.numeric(ymd('2007-03-25')),
            ymin = -Inf, ymax = Inf, alpha = 0.005,
            fill = 'darkturquoise') +
  
  # Forecast Portion
  annotate(x = ymd('2007-05-13'), y = (10.5e7),
           'text',  label = 'Forecast\nPortion', size = 2.8) +
  geom_rect(xmin = as.numeric(ymd('2007-03-26')),
            xmax = as.numeric(ymd('2007-07-01')),
            ymin = -Inf, ymax = Inf, alpha = 0.01,
            fill = 'cornflowerblue')
```

__A very important remark:__ as you can see, I'm not showing 2004 data in the plot. This is because, whenever you include a lag variable in your model, the first period used to calculate the lag __"drop off"__ the dataset and won't be available for modelling. In the case of __a yearly lag__, all observations __"shift" one year ahead__, and as there are no sales recorder for 2003, this  results in the first 52 weeks being dropped from the analysis.


### Feature creation 

Now I can start to incorporate the findings from the Time Series exploration in my feature variables. I do that by creating:

- __Trend__ features: (a _trend_ and _trend squared_ ). this is done with a simple numeric index to control for the upward trend and the potential non-linear trend.

- __Lag__ features: (a _lag_13_ and _lag_52_) to capture the observed correlation of revenue with its quarterly and yearly seasonal lags.

- __Seasonal__ feature to deal with the _2-week-on, 2-week-off_ purchase frequency

```{r}
model_data_tbl <- 
  revenue_tbl %>% 
  mutate(trend       = 1:nrow(revenue_tbl),
         trend_sqr   = trend^2,
         rev_lag_13  = lag(revenue, n = 13),
         rev_lag_52  = lag(revenue, n = 52),
         season      = case_when(revenue == 0 ~ 0,
                                 TRUE ~ 1)
        ) %>% 
 filter(!is.na(rev_lag_52))
```

The next step is to create the __train__, __test__ and __forecast__ data frames. 

As a matter of fact, the __test data__ set is __not strictly required__  because `H2O` allows for __multi-fold cross validation__ to be automatically implemented. 
 
However, as hinted at in the previous paragraph, I'm "carving off" a __test set__ from the train data for the sake of evaluating and comparing the __in-sample performance__ of the fitted models.

```{r}
train_tbl <- 
  model_data_tbl %>% 
  filter(order_date <= "2007-03-19") 

test_tbl <- 
  model_data_tbl %>%
  filter(order_date >= "2006-10-02" &
           order_date <= "2007-03-19") 
```

```{r, collapse=TRUE}
train_tbl %>% head()
```

The main consideration when creating a __forecast data set__ revolves around making calculated __assumptions__ on the likely values and level for the predictor variables. 

- When it comes to the `trend` features, I simply select them from the _model_data_tbl_ dataset. They are based on the numeric index and are fine as they are. 

 - Given that weeks with orders are almost all aligned year in, year out (remember the exploratory analysis?) I am setting `season` and `rev_lag_52` equal to their values a year earlier (52 weeks earlier)
 
- The value of `rev_lag_13` is set equal to its value during the previous quarter (i.e. Q1 2007). 
```{r}
forecast_tbl <- 
  model_data_tbl %>% 
  filter(order_date > "2007-03-19") %>%
  select(order_date:trend_sqr) %>% 
  cbind(season     = model_data_tbl %>%
               filter(between(order_date,
                              as.Date("2006-03-27"),
                              as.Date("2006-06-19"))) %>% select(season),
        rev_lag_52 = model_data_tbl %>%
               filter(between(order_date,
                              as.Date("2006-03-27"),
                              as.Date("2006-06-19"))) %>% select(rev_lag_52),
        rev_lag_13 = model_data_tbl %>%
               filter(between(order_date,
                              as.Date("2006-12-25"),
                              as.Date("2007-03-19"))) %>% select(rev_lag_13)
         )
```

```{r, collapse=TRUE}
forecast_tbl %>% head()
```


### Modelling with H2O

Finally, I am ready to start modelling!

`H2O` is a high performance, open source library for machine learning applications and works on distributed processing, which makes it suitable for smaller in-memory project and can quikly scale up with external processing power for larger undertaking.  

It's Java-based, has dedicated interfaces with both _R_ and _Python_ and incorporates many supervised and unsupervised machine learning models. In this project I am focusing in particular on 4 algorithms:

- __Generalised Linear Model (GLM)__

- __Random Forest (RF)__

- __Gradient Boosting Machine (GBM)__

- I'm also using the __AutoML__ facility and use the leader model to compare performance

First things first: start a `H2O` instance!

When R starts H2O through the `h2o.init` command, I can specify the size of the memory allocation pool cluster. To speed things up a bit, I set it to "16G". 
```{r}
h2o.init(max_mem_size = "16G")
```

I also prefer to switch off the progress bar as in some cases the output message can be quite verbose and lengthy. 
```{r}
h2o.no_progress()
```

The next step is to arrange __response__ and __predictor__ variables sets. For regression to be performed, you need to ensure that the response variable is NOT a factor (otherwise `H2O` will carry out a classification).
```{r}
# response variable
y <- "revenue"

# predictors set: remove response variable and order_date from the set
x <- setdiff(names(train_tbl %>% as.h2o()), c(y, "order_date"))
```

### A random forest

I am going to start by fitting a `random forest`. 

Note that I'm including the `nfolds` parameter. Whenever specified, this parameter enables cross-validation to be carried out without the need for a `validation_frame` - if set to 5 for instance, it will perform a 5-fold cross-validation. 

If you want to use a specific validation frame, simply set `nfolds == 0`, in which case a `validation_frame` argument can be specified (in my case it would be the `test_tbl`) and used for early stopping of individual models and of the grid searches.

I'm also using some of the control parameters to handle the model's running time: 

- I'm setting `stopping_metric` to `RMSE` as the error metric for early stopping (the model will stop building new trees when the metric ceases to improve)

- With `stopping_rounds` I'm specifying the number of training rounds before early stopping is considered

- I'm using `stopping_tolerance` to set minimal improvement needed for the training process to continue


```{r}
# random forest model
rft_model <- 
  h2o.randomForest(
    x = x, 
    y = y, 
    training_frame = train_tbl %>% as.h2o(),
    nfolds = 10,
    ntrees = 500,
    stopping_metric = "RMSE",
    stopping_rounds = 10,
    stopping_tolerance = 0.005,
    seed = 1975
  )
```

I now visualise the variable importance with `h2o.varimp_plot`, which returns a plot with the ranked contribution of each variable, normalised to a scale between 0 and 1.

```{r}
rft_model %>% h2o.varimp_plot()
```

The `lag_52` variable is the most important in the model, followed by `season` and the other lag variable, `lag_13`. On the other hand, neither of the trend variables seem to contribute strongly to our random forest model. 


The `model_summary` function grants access to information about the models parameters. 
```{r, collapse=TRUE}
rft_model@model$model_summary
```

Here we can see that the random forest only uses 26 out of a maximum of 500 trees that it was allowed to estimate (I set this with the `ntrees` parameter). We can also gauge that the tree depth ranges from 7 to 14 (not a particularly deep forest) and that the number of leaves per tree ranges from 12 to 45.

Last but not least, I can review the model's performance with `h2o.performance`
```{r, collapse=TRUE}
h2o.performance(rft_model, newdata = test_tbl %>% as.h2o())
```
The model achieves a high `R^2` of _97.4%_, which means that the variation in the feature variables explains almost all the variability of the response variable. 

On the other hand, `RMSE` appears to be quite large! High values of RMSE can be due to the presence of small number of high error predictions (like in the case of outliers), and this should not surprise given the choppy nature of the response variable. 

When it comes to error based metric like RMSE, MAE, MSE, etc., there is no absolute value of good or bad as they are _expressed in the unit of Response Variable_. Usually, you want to achieve a smaller `RMSE` as this translates into a higher predictive power but for this project I will simply use this metric to compare the relative performance of the different model.

### Extend to many models

Let's generalise the performance assessment in a programmatic way to compute, assess and compare multiple models in one go. 

First, I fit a few more models and make sure I'm enabling `cross-validation ` for all of them. Note that for the _GBM_ I'm specify the same parameters I used for the _random forest_ but there is an extensive array of parameters that you can use to control several aspects of the model's estimation (I will not touch on these as it's outside the scope of this project)

```{r}
# gradient boosting machine model
gbm_model <-  
  h2o.gbm(
    x = x, 
    y = y, 
    training_frame = as.h2o(train_tbl),
    nfolds = 10,
    ntrees = 500,
    stopping_metric = "RMSE",
    stopping_rounds = 10,         
    stopping_tolerance = 0.005,
    seed = 1975
  )

# geleralised linear model (a.k.a. elastic net model)
glm_model <- 
  h2o.glm(
    x = x, 
    y = y, 
    training_frame = as.h2o(train_tbl),
    nfolds = 10,
    family = "gaussian",
    seed = 1975
  )
```


I'm also running the very handy `automl` function that enables for multiple models to be fitted and grid-search optimised. Like I did for the other models, I can specify a series of parameters to guide the function, like several `stopping` metrics, and `max_runtime_secs` to save on computing time.
```{r}
automl_model <-
  h2o.automl(
    x = x,
    y = y,
    training_frame     = as.h2o(train_tbl),
    nfolds             = 5,
    stopping_metric    = "RMSE",
    stopping_rounds    = 10,
    stopping_tolerance = 0.005,
    max_runtime_secs   = 60,
    seed               = 1975
 )
```

Checking the leader board will show the fitted models
```{r, collapse=TRUE}
automl_model@leaderboard
```

As you can see the top model is a __Gradient Boosting Machine__ model. There are also a couple of __Deep Learning__ models and a __stacked ensemble__, `H2O` take on the __Super Learner__.

```{r, eval=TRUE, include=FALSE}
aml_model <- automl_model@leader
```

## Performance assessment

First, I save all models in one folder so that I can access them and process performance metrics programmatically through a series of functions

```{r, eval=FALSE, include=TRUE}
# set path to get around model path being different from project path
path = "../02_models/final/"

# Save GLM model
h2o.saveModel(glm_model, path)

# Save RF model
h2o.saveModel(rft_model, path)

# Save GBM model
h2o.saveModel(gbm_model, path)

# Extracs and save the leader autoML model
aml_model <- automl_model@leader

h2o.saveModel(aml_model, path)
```

```{r, eval=FALSE, include=FALSE}
# set path to get around model path being different from project path
path = "C:/Users/LENOVO/Desktop/PROJECTS/Customer_Analytics/06_Demand_Forecast/02_models/final/"
# Save GLM model
h2o.saveModel(glm_model, path)

# Save RF model
h2o.saveModel(rft_model, path)

# Save GBM model
h2o.saveModel(gbm_model, path)

# Save autoML model
h2o.saveModel(aml_model, path)
```


### Variable importance plots

Let's start with the variable importance plots. Earlier on I used  plotting functionality with the `random forest` model but now I want to plot them all in one go so that I can compare and contrast the results. 

There are many libraries (like _IML_, _PDP_, _VIP_, and _DALEX_ to name the more popular ones) that help with __Machine Learning model interpretability__, __feature explanation__ and __general performance assessment__. In this project I am using the `vip` package.

One of the main advantages of these libraries is their compatibility with other R packages such as `gridExtra`.

```{r}
p_glm <- vip(glm_model) + ggtitle("GLM")
p_rft <- vip(rft_model) + ggtitle("RF")
p_gbm <- vip(gbm_model) + ggtitle("GBM")
p_aml <- vip(aml_model) + ggtitle("AML")

grid.arrange(p_glm, p_rft, p_gbm, p_aml, nrow = 2)
```
`Seasonality` and previous revenue levels (`lags`) come through at the top 3 stronger drives in almost all models (the only exception being _GBM_). Conversely, none of the models find `trend` and its `squared` counterpart to be a strong explainer of variation in the response variable.

### Performance metrics

As I've shown earlier with the `random forest` model, the `h2o.performance` function shows a single model's performance at a glance (here for example I check the performance of the `GBM` model)
```{r, collapse=TRUE}
perf_gbm_model <- 
  h2o.performance(gbm_model, newdata = as.h2o(test_tbl))

perf_gbm_model
```

However, to assess and compare the models performance I am going to focus on  `RMSE` and `R^2^`. 

All performance metrics can be pulled at once using the `h20.metric` function, which for some reason does not seem to work with `H2ORegressionMetrics` objects. 
```{r, eval=FALSE, }
perf_gbm_model %>% 
  h2o.metric()

## Error in paste0("No ", metric, " for ",
## class(object)) : argument "metric" is missing, with
##  no default
```

Also the error message is not particularly helpful in this case as the "metric" argument is optional and should return all metrics by default. The issue seems to have to do with performing a regression as it actually works fine with `H2OClassificationMetrics` objects. 

Luckily,  provides some useful helper functions to extract the single metrics individually, which work all right!
```{r, collapse=TRUE}
perf_gbm_model %>% h2o.r2()
perf_gbm_model %>% h2o.rmse()
```

So I will be using these individual helpers to write a little function that runs __predictions for all models__ on the test data and returns a handy tibble to house all the performance metrics.


```{r}
performance_metrics_fct <- function(path, data_tbl) {
    
    model_h2o <- h2o.loadModel(path)
    perf_h2o  <- h2o.performance(model_h2o, newdata = as.h2o(data_tbl)) 
    
    R2   <- perf_h2o %>% h2o.r2()  
    RMSE <- perf_h2o %>% h2o.rmse()
    
    tibble(R2, RMSE)
}
```

Now I can pass this formula to a `map` function from the `purrr` package to iterate calculations and compile `RMSE` and `R^2^` across all models. To correctly identify each model, I also make sure to extract the model's name from the path. 
```{r}
perf_metrics_test_tbl <- fs::dir_info(path = "../02_models/final/") %>%
    select(path) %>%
    mutate(metrics = map(path, performance_metrics_fct, data_tbl = test_tbl),
           path = str_split(path, pattern = "/", simplify = T)[,4] 
                            %>% substr(1,3)) %>%
    rename(model = path) %>% 
    unnest(cols = c(metrics)) 

perf_metrics_test_tbl %>% 
  arrange(desc(R2)) %>% 
  knitr::kable()
```

__All tree-based models__ achieve very high `R^2`, with the autoML model (which is a GBM, remember?) reaching a staggering _99.3%_ and achieves the lowest `RMSE`. The _GLM_ on the other hand scores a __negative R^2__. 

A __negarive R^2__ is not unheard of: the R^2 compares the fit of a model with that of a horizontal straight line and calculates the proportion of the variance explained by the model compared to that of a straight line (the null hypothesis). If the fit is actually worse than just fitting a horizontal line then R-square can be negative.

### Actual vs Predicted plots

Last by not least, to provides an additional and more visual display of the models performance, I’m going to plot the __actual versus predicted__ for all models.

I'm using a function similar to the one I've used to calculate the performance metrics as the basic principle is the same.
```{r}
predict_fct <- function(path, data_tbl) {
    
    model_h2o <- h2o.loadModel(path)
    pred_h2o  <- h2o.predict(model_h2o, newdata = as.h2o(data_tbl)) 
    
    pred_h2o %>% 
      as_tibble() %>% 
      cbind(data_tbl %>% select(order_date))
    
}
```

As I did before, I pass the formula to a `map` function to iterate calculations and compile `prediction` using the `test` data subset across all models.

```{r}
validation_tmp <- fs::dir_info(path = "../02_models/final/") %>%
    select(path) %>%
    mutate(pred = map(path, predict_fct, data_tbl = test_tbl),
           path = str_split(path, pattern = "/", simplify = T)[,4] %>% 
             substr(1,3)) %>%
    rename(model = path) 
```

However, the resulting `validation_tmp` is a nested tibble, with the predictions stored as lists in each cell. 
```{r, collapse=TRUE}
validation_tmp
```

This requires a couple of additional manipulations to get in in a shape that can be used for plotting: unnesting the list, "pivot" the predictions around the `order_date` and add the revenue as `actual`.

```{r}
validation_tbl <- 
    validation_tmp %>% 
    unnest(cols = c(pred)) %>% 
    pivot_wider(names_from = model, 
                values_from = predict) %>%
    cbind(test_tbl %>% 
            select(actual = revenue)) %>% 
    rename(date = order_date)
```

Now, I'm going to write this plotting function directly in _plotly_

```{r}
validation_tbl %>% 
  plot_ly() %>% 
    add_lines(x = ~ date, y = ~ actual, name = 'Actual') %>% 
    add_lines(x = ~ date, y = ~ DRF, name = 'Random Forest', 
              line = list(dash = 'dot')) %>% 
    add_lines(x = ~ date, y = ~ GBM, name = 'Gradient Boosting Machine', 
              line = list(dash = 'dash')) %>% 
    add_lines(x = ~ date, y = ~ AML, name = 'Auto ML', 
              line = list(dash = 'dot')) %>% 
    add_lines(x = ~ date, y = ~ GLM, name = 'Generalised Linear Model', 
              line = list(dash = 'dash')) %>% 
    layout(title = 'Total Weekly Sales - Actual versus Predicted (various models)',
           yaxis = list(title = 'Millions of Dollars'),
           xaxis = list(title = ''),
           legend = list(orientation = 'h')
           )
  
```

With the exception of the GLM model, which is producing a seemingly flat line prediction (remember the negative _R^2_?), all models are capturing well the peaks and troughs in the series. The prediction only start to miss the full extent of the response variation around the last 2 spikes.  



## Forecasting

No need to write any new functions as the `performance_metrics_fct` and `predict_fct`can also be used for the forecasting.

First, I take a look at the performance metrics

```{r}
perf_metrics_cast_tbl <- fs::dir_info(path = "../02_models/final/") %>%
    select(path) %>%
    mutate(metrics = map(path, performance_metrics_fct, data_tbl = forecast_tbl),
           path = str_split(path, pattern = "/", simplify = T)[,4] 
                            %>% substr(1,3)) %>%
    rename(model = path) %>% 
    unnest(cols = c(metrics)) 

perf_metrics_cast_tbl %>% 
  arrange(desc(R2)) %>% 
  kable()
```

Interestingly, a little swap in the top position, with the "manual" GBM performing better in the forecast that the autoML model. The performance metrics worsened for all models when compared to the validation metrics. 

Then, I calculate the forecast...
```{r}
cast_tbl <- fs::dir_info(path = "../02_models/final/") %>%
    select(path) %>%
    mutate(pred = map(path, predict_fct, data_tbl = forecast_tbl),
           path = str_split(path, pattern = "/", simplify = T)[,4] %>% 
             substr(1,3)) %>%
    rename(model = path) %>% 
    unnest(cols = c(pred)) %>% 
    pivot_wider(names_from = model, values_from = predict) %>%
    cbind(forecast_tbl %>% select(actual = revenue)) %>% 
    rename(date = order_date)
```

...and visualise it
```{r}
cast_tbl %>% 
  plot_ly() %>% 
    add_lines(x = ~ date, y = ~ actual, name = 'Actual') %>% 
    add_lines(x = ~ date, y = ~ DRF, name = 'Random Forest', 
              line = list(dash = 'dot')) %>% 
    add_lines(x = ~ date, y = ~ GBM, name = 'Gradient Boosting Machine', 
              line = list(dash = 'dash')) %>% 
    add_lines(x = ~ date, y = ~ AML, name = 'Auto ML', 
              line = list(dash = 'dot')) %>% 
    add_lines(x = ~ date, y = ~ GLM, name = 'Generalised Linear Model', 
              line = list(dash = 'dash')) %>% 
    layout(title = 'Total Weekly Sales - Actual versus Forecast (various models)',
           yaxis = list(title = 'Millions of Dollars'),
           xaxis = list(title = ''),
           legend = list(orientation = 'h')
           )
  
```

Aside from the __GLM__ which unsurprisingly produces a flat line forecast, all models continue to capture well the 2-week-on, 2-week-off purchase pattern. Additionally, all models forecasts fail to capture the full extent of the response variable movements for all 3 spikes, suggesting that there could be another dynamic at play in 2007 that the current predictors are not controlling for.

Taking a closer look at the `forecast_tbl` reveals that, for observations 9 throught to 11, both `season` and `lag_52` predictors are not perfectly alligned with the actual revenue, which explains why all models predict positive revenue for weeks with actual zero sales. 
```{r}
forecast_tbl %>% 
  select(-trend, -trend_sqr) %>% 
  tail(10) %>% 
  kable()
```


One final thing: don't forget to shut-down the `H2O` instance when you're done!
```{r, collapse=TRUE}
h2o.shutdown(prompt = FALSE)
```

## Closing thoughts

In this project I have gone through the various steps needed to build a time series machine learning pipeline and generate a weekly revenue forecast.

In particular, I have carried out a more "traditional" __exploratory time series analysis__ with `TSstudio` and __created a number of predictors__ using the insight I gathered. I've then __trained and validated__ an array of machine learning models with the open source library `H2O`, and __compared the models' accuracy__ using __performance metrics__ and __actual vs predicted__ plots.

## Conclusions

This is just an first stab at producing a weekly revenue forecast and there clearly is plenty of room for improvement. Still, once you have a modelling and forecasting pipeline like this in place, it becomes much easier and faster to create and test several models and different predictors sets. 

The fact that the data series is artificially generated is not ideal as it does not necessarily incorporate dynamics that you would encounter in a real-life dataset. Nonetheless that challenged me to get inventive and made the whole exercise that more enjoyable.

Forecasting total revenue may not be the best strategy and perhaps breaking down the response variable by `product line` or by `country` for instance may lead to improved and more accurate forecasts. This is behond the scope of this project but could well be the subject for a future project!

One thing I take away from this exercise is that `H2O` is __absolutely brilliant__! It's fast, intuitive to set up and has a broad array of customisation options. __AutoML__ is superb, has support with __R__, __Python__ and __Java__ and the fact that anyone can use it free of charge gives the likes of __Google AutoML__ and __AWS SageMaker AutoML__ platform a run for their money! 


### Code Repository
The full R code can be found on [my GitHub profile](https://github.com/DiegoUsaiUK/Customer_Analytics/tree/master/time_series_machine_learning)


### References
* For __H2O__ website [H2O Website](https://www.h2o.ai/)
* For __H2O__ documentation [H2O Documentation](http://docs.h2o.ai/h2o/latest-stable/index.html)
* For a thorough discussion on performing time series analysis and forecasting in R [Hands-On Time Series Analysis with R](https://www.packtpub.com/big-data-and-business-intelligence/hands-time-series-analysis-r)
* For an introduction to TSstudio [Introduction for the TSstudio Package](Introduction for the TSstudio Package)
* For an introduction to [__Machine Learning interpretability__](https://www.h2o.ai/wp-content/uploads/2019/08/An-Introduction-to-Machine-Learning-Interpretability-Second-Edition.pdf)


